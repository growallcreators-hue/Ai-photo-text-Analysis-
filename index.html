<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Vision & Analysis Tool</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f7fafc;
        }
        .container-card {
            max-width: 90%;
            margin: 0 auto;
            padding: 1.5rem;
            background-color: #ffffff;
            border-radius: 1.5rem;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }
        .input-group label {
            font-weight: 600;
            color: #1a202c;
            margin-bottom: 0.5rem;
            display: block;
        }
        .btn-primary {
            background-color: #4f46e5;
            color: white;
            padding: 0.75rem 1.5rem;
            border-radius: 0.75rem;
            font-weight: 600;
            transition: background-color 0.2s;
        }
        .btn-primary:hover {
            background-color: #4338ca;
        }
        .loading-spinner {
            border: 4px solid rgba(255, 255, 255, 0.3);
            border-top: 4px solid #ffffff;
            border-radius: 50%;
            width: 1.5rem;
            height: 1.5rem;
            animation: spin 1s linear infinite;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body class="min-h-screen py-8 bg-gray-100">

    <div class="container-card">
        <h1 class="text-3xl font-bold text-center text-gray-800 mb-6">AI Vision & Analysis Tool</h1>
        <p class="text-center text-gray-500 mb-8">Upload an image, extract text, and ask AI with Google Search for analysis.</p>

        <!-- Main Application Grid --><div class="grid grid-cols-1 lg:grid-cols-2 gap-8">

            <!-- 1. Image Upload and Extraction Panel --><div class="p-6 bg-blue-50 border border-blue-200 rounded-xl shadow-inner">
                <h2 class="text-xl font-semibold mb-4 text-blue-700">Step 1: Extract Text from Image</h2>

                <div class="input-group mb-4">
                    <label for="imageUpload" class="text-sm">Upload Image (screenshot, photo, etc.)</label>
                    <input type="file" id="imageUpload" accept="image/*" class="w-full text-sm text-gray-500 file:mr-4 file:py-2 file:px-4 file:rounded-full file:border-0 file:text-sm file:font-semibold file:bg-blue-100 file:text-blue-700 hover:file:bg-blue-200" onchange="handleImageUpload(event)">
                </div>

                <div id="imagePreviewContainer" class="mb-4 hidden">
                    <img id="imagePreview" class="w-full h-auto max-h-64 object-contain rounded-lg border border-gray-200 p-2 bg-white" alt="Image Preview">
                </div>

                <button id="extractBtn" class="w-full btn-primary flex items-center justify-center space-x-2 disabled:bg-gray-400" onclick="extractTextFromImage()" disabled>
                    <span id="extractSpinner" class="hidden loading-spinner"></span>
                    <span id="extractBtnText">Extract Text (OCR)</span>
                </button>

                <div class="input-group mt-6" id="extractedTextGroup" style="display: none;">
                    <label for="extractedText" class="text-sm">Extracted Text</label>
                    <textarea id="extractedText" rows="6" readonly class="w-full p-3 border border-gray-300 rounded-lg focus:ring-blue-500 focus:border-blue-500 text-sm bg-white"></textarea>
                    <button id="copyBtn" class="mt-2 w-full text-sm py-2 px-4 rounded-lg bg-green-500 text-white font-semibold hover:bg-green-600 transition" onclick="copyText()">Copy Text</button>
                </div>
            </div>

            <!-- 2. Query Submission and Answer Panel --><div class="p-6 bg-purple-50 border border-purple-200 rounded-xl shadow-inner">
                <h2 class="text-xl font-semibold mb-4 text-purple-700">Step 2: Ask AI for Analysis</h2>

                <div class="input-group mb-4">
                    <label for="userQuery" class="text-sm">Your Question or Context (e.g., "What does this mean?" or "Translate this.")</label>
                    <textarea id="userQuery" rows="3" class="w-full p-3 border border-gray-300 rounded-lg focus:ring-purple-500 focus:border-purple-500 text-sm" placeholder="Paste extracted text here, or type your question..."></textarea>
                </div>

                <button id="askAIBtn" class="w-full btn-primary flex items-center justify-center space-x-2 disabled:bg-gray-400" onclick="askAI()" disabled>
                    <span id="aiSpinner" class="hidden loading-spinner"></span>
                    <span id="askAIBtnText">Ask AI & Get Analysis</span>
                </button>

                <!-- TTS Controls --><div class="mt-4 p-3 bg-yellow-100 border border-yellow-300 rounded-lg">
                    <div class="flex items-center justify-between mb-2">
                        <label for="ttsSpeed" class="text-sm font-semibold text-yellow-800">Speech Speed:</label>
                        <span id="ttsSpeedDisplay" class="text-sm font-medium text-yellow-800">1.0x</span>
                    </div>
                    <input type="range" id="ttsSpeed" min="0.5" max="2.0" value="1.0" step="0.1" class="w-full h-2 bg-yellow-200 rounded-lg appearance-none cursor-pointer range-lg" oninput="updateTtsSpeedDisplay()">
                    <button id="readAnalysisBtn" class="w-full text-sm py-2 px-4 rounded-lg bg-yellow-500 text-white font-semibold hover:bg-yellow-600 transition flex items-center justify-center space-x-2 mt-3 disabled:bg-gray-400" onclick="readAnalysisText()" disabled>
                        <span id="ttsSpinner" class="hidden loading-spinner !border-t-yellow-500 !border-white"></span>
                        <span id="ttsBtnText">Read Analysis Aloud</span>
                    </button>
                </div>

                <div class="mt-6">
                    <label class="text-sm font-semibold text-purple-700 mb-2 block">AI Analysis & Answer</label>
                    <div id="aiAnswer" class="min-h-[100px] p-4 bg-white border border-gray-300 rounded-lg text-gray-700 whitespace-pre-wrap">
                        AI's answer will appear here. The model is grounded with Google Search for up-to-date context.
                    </div>
                    <div id="sourcesContainer" class="mt-3 text-xs text-gray-500"></div>
                </div>
            </div>

        </div>
    </div>

    <!-- Custom Modal for Notifications --><div id="customModal" class="hidden fixed inset-0 bg-gray-600 bg-opacity-50 flex items-center justify-center z-50">
        <div class="bg-white p-6 rounded-lg shadow-xl max-w-sm w-full">
            <h3 id="modalTitle" class="text-lg font-bold mb-2 text-gray-800">Notice</h3>
            <p id="modalMessage" class="text-sm text-gray-600 mb-4">Message content.</p>
            <button class="w-full py-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600" onclick="document.getElementById('customModal').classList.add('hidden')">Close</button>
        </div>
    </div>


    <script>
        // --- Essential Globals (MUST be defined for Canvas environment) ---
        const appId = typeof __app_id !== 'undefined' ? __app_id : 'default-app-id';
        const firebaseConfig = typeof __firebase_config !== 'undefined' ? JSON.parse(__firebase_config) : {};
        const apiKey = ""; // Canvas environment provides API Key automatically

        // --- Application State and UI elements ---
        let base64Image = null;
        let extractedTextContent = "";
        let analysisTextContent = "";
        let currentTtsSpeed = 1.0; // Default TTS speed

        const VLM_API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key=${apiKey}`;
        const TTS_API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent?key=${apiKey}`;

        const ui = {
            imageUpload: document.getElementById('imageUpload'),
            imagePreviewContainer: document.getElementById('imagePreviewContainer'),
            imagePreview: document.getElementById('imagePreview'),
            extractBtn: document.getElementById('extractBtn'),
            extractSpinner: document.getElementById('extractSpinner'),
            extractBtnText: document.getElementById('extractBtnText'),
            extractedText: document.getElementById('extractedText'),
            extractedTextGroup: document.getElementById('extractedTextGroup'),
            copyBtn: document.getElementById('copyBtn'),
            userQuery: document.getElementById('userQuery'),
            askAIBtn: document.getElementById('askAIBtn'),
            aiSpinner: document.getElementById('aiSpinner'),
            askAIBtnText: document.getElementById('askAIBtnText'),
            aiAnswer: document.getElementById('aiAnswer'),
            sourcesContainer: document.getElementById('sourcesContainer'),
            readAnalysisBtn: document.getElementById('readAnalysisBtn'),
            ttsSpinner: document.getElementById('ttsSpinner'),
            ttsBtnText: document.getElementById('ttsBtnText'),
            ttsSpeed: document.getElementById('ttsSpeed'),
            ttsSpeedDisplay: document.getElementById('ttsSpeedDisplay'),
            modal: document.getElementById('customModal'),
            modalTitle: document.getElementById('modalTitle'),
            modalMessage: document.getElementById('modalMessage'),
        };

        // Initialize TTS speed display
        ui.ttsSpeed.value = currentTtsSpeed;
        ui.ttsSpeedDisplay.textContent = `${currentTtsSpeed.toFixed(1)}x`;


        // --- Utility Functions ---

        function showModal(title, message) {
            ui.modalTitle.textContent = title;
            ui.modalMessage.textContent = message;
            ui.modal.classList.remove('hidden');
        }

        function base64ToArrayBuffer(base64) {
            const binaryString = atob(base64);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        }

        // Converts raw PCM data (from TTS API) into a WAV file blob for playback
        function pcmToWav(pcm16, sampleRate = 24000) {
            const numChannels = 1;
            const bytesPerSample = 2; // 16-bit PCM
            const dataLength = pcm16.length * bytesPerSample;
            const buffer = new ArrayBuffer(44 + dataLength);
            const view = new DataView(buffer);

            // RIFF chunk
            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + dataLength, true);
            writeString(view, 8, 'WAVE');

            // FMT chunk
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true); // PCM format
            view.setUint16(22, numChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * numChannels * bytesPerSample, true); // Byte rate
            view.setUint16(32, numChannels * bytesPerSample, true);
            view.setUint16(34, bytesPerSample * 8, true); // Bits per sample (16)

            // Data chunk
            writeString(view, 36, 'data');
            view.setUint32(40, dataLength, true);

            // Write PCM data
            let offset = 44;
            for (let i = 0; i < pcm16.length; i++, offset += 2) {
                view.setInt16(offset, pcm16[i], true);
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }

        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

        async function exponentialBackoffFetch(url, options, maxRetries = 5) {
            for (let i = 0; i < maxRetries; i++) {
                try {
                    const response = await fetch(url, options);
                    
                    // If response is not OK (status 200-299)
                    if (!response.ok) {
                        // Check if it's a 429 (Too Many Requests) for potential retry
                        if (response.status === 429 && i < maxRetries - 1) {
                            // Retry on 429 - Fall through to delay
                        } else {
                            // Throw error for all other non-OK status codes or last 429 attempt
                            const errorText = await response.text().catch(() => 'Failed to read response body');
                            throw new Error(`API call failed with status ${response.status}: ${errorText}`);
                        }
                    } else {
                        // Response is OK (200-299), return it immediately
                        return response;
                    }

                } catch (error) {
                    // This catches network errors or the error thrown above
                    if (i === maxRetries - 1) throw error; 
                }
                const delay = Math.pow(2, i) * 1000 + Math.random() * 1000;
                await new Promise(resolve => setTimeout(resolve, delay));
            }
            throw new Error("Fetch failed after multiple retries.");
        }


        // --- Step 1 Logic: Image Upload and Text Extraction (OCR/VLM) ---

        function handleImageUpload(event) {
            const file = event.target.files[0];
            if (file) {
                const reader = new FileReader();
                reader.onload = function(e) {
                    base64Image = e.target.result.split(',')[1];
                    
                    ui.imagePreview.src = e.target.result;
                    ui.imagePreviewContainer.classList.remove('hidden');

                    // Reset states
                    ui.extractBtn.disabled = false;
                    extractedTextContent = "";
                    analysisTextContent = "";
                    ui.extractedText.value = "";
                    ui.aiAnswer.textContent = "AI's answer will appear here. The model is grounded with Google Search for up-to-date context.";
                    ui.extractedTextGroup.style.display = 'none';
                    ui.askAIBtn.disabled = true;
                    ui.readAnalysisBtn.disabled = true;
                };
                reader.readAsDataURL(file);
            }
        }

        async function extractTextFromImage() {
            if (!base64Image) {
                showModal("Error", "Please upload an image first.");
                return;
            }

            // Show loading state
            ui.extractBtn.disabled = true;
            ui.extractSpinner.classList.remove('hidden');
            ui.extractBtnText.textContent = 'Extracting...';
            ui.extractedText.value = 'Analyzing image for text...';

            const payload = {
                contents: [{
                    parts: [
                        { text: "You are an OCR and text extraction utility. Extract all distinct pieces of text you can read from this image. Return ONLY the raw text, line by line, without any formatting, numbering, commentary, or explanation." },
                        {
                            inlineData: {
                                mimeType: ui.imageUpload.files[0].type,
                                data: base64Image
                            }
                        }
                    ]
                }],
            };

            try {
                const response = await exponentialBackoffFetch(VLM_API_URL, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                const result = await response.json();
                const text = result.candidates?.[0]?.content?.parts?.[0]?.text || "Could not extract text. Please try a clearer image.";

                // Update state and UI
                extractedTextContent = text.trim();
                ui.extractedText.value = extractedTextContent;
                ui.extractedTextGroup.style.display = 'block';
                const extractionFailed = extractedTextContent.startsWith("Could not extract text");
                ui.askAIBtn.disabled = extractionFailed;
                
                if (!extractionFailed) {
                    ui.userQuery.value = extractedTextContent;
                }

            } catch (error) {
                console.error("Extraction API Error:", error);
                showModal("Extraction Failed", "An error occurred communicating with the AI. Check console for details.");
                extractedTextContent = "Error: Failed to extract text.";
                ui.extractedText.value = extractedTextContent;
                ui.askAIBtn.disabled = true;
            } finally {
                // Reset loading state
                ui.extractSpinner.classList.add('hidden');
                ui.extractBtnText.textContent = 'Extract Text (OCR)';
                ui.extractBtn.disabled = false;
            }
        }

        function copyText() {
            if (extractedTextContent) {
                ui.extractedText.select();
                try {
                    document.execCommand('copy');
                    showModal("Success", "Extracted text copied to clipboard!");
                } catch (err) {
                    console.error('Copy failed:', err);
                    showModal("Error", "Failed to copy text. Please copy it manually from the box.");
                }
            }
        }


        // --- Step 2 Logic: AI Analysis (LLM with Grounding) ---

        async function askAI() {
            const userQuery = ui.userQuery.value.trim();
            if (!userQuery) {
                showModal("Error", "Please enter a question or paste extracted text in the analysis box.");
                return;
            }

            // Show loading state
            ui.askAIBtn.disabled = true;
            ui.readAnalysisBtn.disabled = true; // Disable TTS while analyzing
            ui.aiSpinner.classList.remove('hidden');
            ui.askAIBtnText.textContent = 'Analyzing...';
            ui.aiAnswer.textContent = 'Thinking and searching for an answer...';
            ui.sourcesContainer.innerHTML = '';
            analysisTextContent = '';

            const systemPrompt = "You are an intelligent, grounded analysis engine. Your task is to process the user's input, which often contains raw, extracted text, and provide a comprehensive, detailed, and helpful answer. Use the provided Google Search tool to ground your response with up-to-date facts when necessary.";

            const payload = {
                contents: [{ parts: [{ text: userQuery }] }],
                tools: [{ "google_search": {} }],
                systemInstruction: {
                    parts: [{ text: systemPrompt }]
                },
            };

            try {
                const response = await exponentialBackoffFetch(VLM_API_URL, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                const result = await response.json();
                const candidate = result.candidates?.[0];

                if (candidate && candidate.content?.parts?.[0]?.text) {
                    const text = candidate.content.parts[0].text;
                    ui.aiAnswer.textContent = text;
                    analysisTextContent = text; // Store for TTS

                    // Extract and display grounding sources
                    let sourcesHtml = '';
                    const groundingMetadata = candidate.groundingMetadata;
                    if (groundingMetadata && groundingMetadata.groundingAttributions) {
                        const sources = groundingMetadata.groundingAttributions
                            .map(attribution => ({
                                uri: attribution.web?.uri,
                                title: attribution.web?.title,
                            }))
                            .filter(source => source.uri && source.title);

                        if (sources.length > 0) {
                            sourcesHtml = '<p class="font-semibold text-gray-700 mb-1">Sources:</p><ul class="list-disc list-inside space-y-1">';
                            sources.forEach((source, index) => {
                                sourcesHtml += `<li><a href="${source.uri}" target="_blank" class="text-blue-600 hover:text-blue-800 transition">${source.title}</a></li>`;
                            });
                            sourcesHtml += '</ul>';
                        }
                    }
                    ui.sourcesContainer.innerHTML = sourcesHtml;
                    
                    // Enable TTS
                    ui.readAnalysisBtn.disabled = false;

                } else {
                    ui.aiAnswer.textContent = "Sorry, I couldn't answer that question. The API response was empty or malformed.";
                }

            } catch (error) {
                console.error("Analysis API Error:", error);
                showModal("Analysis Failed", "An error occurred communicating with the AI analysis engine. Check console for details.");
                ui.aiAnswer.textContent = "Error: Failed to get analysis. Please try again.";
            } finally {
                // Reset loading state
                ui.aiSpinner.classList.add('hidden');
                ui.askAIBtnText.textContent = 'Ask AI & Get Analysis';
                ui.askAIBtn.disabled = false;
            }
        }
        
        // --- TTS Speed Control ---
        function updateTtsSpeedDisplay() {
            currentTtsSpeed = parseFloat(ui.ttsSpeed.value);
            ui.ttsSpeedDisplay.textContent = `${currentTtsSpeed.toFixed(1)}x`;
        }

        // --- TTS Logic ---
        function resetTtsState() {
            ui.ttsSpinner.classList.add('hidden');
            ui.ttsBtnText.textContent = 'Read Analysis Aloud';
            ui.readAnalysisBtn.disabled = false;
            ui.ttsSpeed.disabled = false;
        }

        async function readAnalysisText() {
            if (!analysisTextContent) {
                showModal("Error", "No analysis result to speak. Click 'Ask AI' first.");
                return;
            }

            // Show loading state
            ui.readAnalysisBtn.disabled = true;
            ui.ttsSpeed.disabled = true; // Disable speed control during playback
            ui.ttsSpinner.classList.remove('hidden');
            ui.ttsBtnText.textContent = 'Speaking... (Please wait)';

            const payload = {
                contents: [{
                    parts: [{ text: analysisTextContent }]
                }],
                generationConfig: {
                    responseModalities: ["AUDIO"],
                    speechConfig: {
                        voiceConfig: {
                            pitch: 0, // Default pitch
                            speakingRate: currentTtsSpeed, // Apply user-selected speed
                            prebuiltVoiceConfig: { voiceName: "Kore" } 
                        }
                    }
                },
                model: "gemini-2.5-flash-preview-tts"
            };

            try {
                const response = await exponentialBackoffFetch(TTS_API_URL, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                const result = await response.json();
                const part = result?.candidates?.[0]?.content?.parts?.[0];
                const audioData = part?.inlineData?.data;
                const mimeType = part?.inlineData?.mimeType;
                
                if (audioData && mimeType && mimeType.startsWith("audio/L16")) {
                    const sampleRateMatch = mimeType.match(/rate=(\d+)/);
                    const sampleRate = sampleRateMatch ? parseInt(sampleRateMatch[1], 10) : 24000;
                    
                    const pcmData = base64ToArrayBuffer(audioData);
                    const pcm16 = new Int16Array(pcmData);
                    const wavBlob = pcmToWav(pcm16, sampleRate);
                    const audioUrl = URL.createObjectURL(wavBlob);
                    
                    const audio = new Audio(audioUrl);
                    
                    // Set up handlers before playing
                    audio.onended = () => {
                        resetTtsState();
                    };
                    audio.onerror = (e) => {
                        console.error("Audio playback error:", e);
                        showModal("Audio Error", "There was an issue playing the audio. Check console for details.");
                        resetTtsState();
                    };

                    // Try to play audio
                    audio.play().catch(e => {
                        console.error("Audio play failed (Promise rejection):", e);
                        showModal("Playback Blocked", "The browser might have blocked automatic playback. Please ensure the analysis text is short and try again.");
                        resetTtsState();
                    });
                } else {
                    showModal("TTS Failed", "No valid audio data received or format unsupported from the server.");
                    resetTtsState();
                }

            } catch (error) {
                console.error("TTS API Error:", error);
                showModal("TTS Failed", "An error occurred communicating with the Text-to-Speech service. Check console for details.");
                resetTtsState();
            }
        }
    </script>

</body>
</html>
